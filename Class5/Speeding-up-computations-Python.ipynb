{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Speeding-Up Computations\n",
    "**Please Note**: Approaches presented in each section are listed in no particular order -- some (or many) may work better for your use case; performance of approach may differ on data set, where it's hosted, computing resources, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import dask.dataframe as dd\n",
    "import inspect\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Timing results](https://stackoverflow.com/questions/17579357/time-time-vs-timeit-timeit): `%%time` and `%%timeit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speeding-up Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"https://s3.amazonaws.com/h2o-airlines-unpacked/year2012.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 4.88 s, total: 23.7 s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(filepath_or_buffer=file_name,\n",
    "                 encoding='latin-1')\n",
    "# df = pd.read_csv(\"../Class3/2012.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6096762, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 570 ms, sys: 5.7 ms, total: 575 ms\n",
      "Wall time: 572 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UA     531245\n",
       "F9      79255\n",
       "EV     740855\n",
       "OO     617756\n",
       "FL     218162\n",
       "YV     133976\n",
       "HA      74109\n",
       "AS     147569\n",
       "VX      54742\n",
       "DL     726879\n",
       "B6     229056\n",
       "MQ     473140\n",
       "US     404263\n",
       "AA     525220\n",
       "WN    1140535\n",
       "Name: UniqueCarrier, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df[\"UniqueCarrier\"].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. `pandas` - Read File in Chunks\n",
    "Reference: https://towardsdatascience.com/why-and-how-to-use-pandas-with-large-data-9594dda2ea4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.1 s, sys: 5.86 s, total: 25.9 s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create an object for iteration over, to spead-up read-ing process:\n",
    "df_chunked = pd.read_csv(filepath_or_buffer=file_name,\n",
    "                         encoding='latin-1',\n",
    "                         chunksize=1000000)\n",
    "\n",
    "# Create a list to store data set chunks:\n",
    "chunk_list = []\n",
    "\n",
    "\n",
    "# Each chunk is in df format\n",
    "for chunk in df_chunked:  \n",
    "    chunk_list.append(chunk)\n",
    "else:\n",
    "    # concat the list into dataframe \n",
    "    df = pd.concat(chunk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. (If Possible) Perform in-database Computations\n",
    "- **Approach** (if possible), as we did in [Class 2](https://goo.gl/JkLxHq):\n",
    "  1. Connect script to database\n",
    "  2. Perform aggregations and variable transformations in-database\n",
    "  3. Send results back to script\n",
    "- **Please note**, this may not be possible, because:\n",
    "  - data is not in database to begin with, or\n",
    "  - time to put data into database to aggegate in, is too time consuming, or\n",
    "  - you're querying a production database -- and running this query may slow-down performance of services running in production that depend on this database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. `dask` -- Read File as is\n",
    "[Documentation](http://docs.dask.org/en/latest/) and more [examples](https://www.analyticsvidhya.com/blog/2018/08/dask-big-datasets-machine_learning-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.91 s ± 220 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dd.read_csv(file_name, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 185 ms, sys: 63.9 ms, total: 249 ms\n",
      "Wall time: 2.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_dask = dd.read_csv(file_name,\n",
    "                      encoding='latin-1',\n",
    "                      assume_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. `pySpark` -- Read File as is\n",
    "[Documentation](https://spark.apache.org/docs/2.2.0/) and more examples of Airlines data set [analysis](https://github.com/goldshtn/spark-workshop/blob/master/scala/lab2-airlines.md) in `pySpark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go this [Databricks notebook](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7565198439641616/2474892018501188/7912616974346672/latest.html) to view `pySpark` code that reads-in Airlines dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time duration (seconds) = 90 seconds (0.02 to get `file_name`, 60.02 to read into DBFS and 29.93 to read into notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the code, you'll need a Databricks account (see class slides on instructions) to `Import Notebook` into. (`Import Notebook` prompt is at top-right of screen of the [Databricks notebook](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7565198439641616/2474892018501188/7912616974346672/latest.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for reading-in Airlines data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speeding-Up Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `pandas`\n",
    "Suggested (non-exhaustive) list of approaches:\n",
    "- Drop [unnecessary columns](https://realpython.com/python-data-cleaning-numpy-pandas/#dropping-columns-in-a-dataframe)\n",
    "- Create a better index for [faster subsetting](https://realpython.com/python-data-cleaning-numpy-pandas/#changing-the-index-of-a-dataframe)\n",
    "- Type optimization of variables in dataset, per [this](https://www.dataquest.io/blog/pandas-big-data/) and [this](https://medium.com/@vincentteyssier/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e) blog post\n",
    "- Saving intermediate results in HDF5 store, per Wes McKinney's book [Python for Data Analysis](http://wesmckinney.com/pages/book.html) and this [blog post](https://realpython.com/fast-flexible-pandas/#prevent-reprocessing-with-hdfstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### a. `for-loop` versus `apply()` versus `applymap()` versus `cut()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal** [as in Class 2](https://github.com/ikukuyeva/Stats-404-W19-Statistical-Computing/blob/master/Class2/Intro-to-pandas.ipynb): Convert delays from minutes to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.shape[0]\n",
    "dep_delay_hr = [None] * num_rows\n",
    "col_index = np.where(df.columns == 'DepDelay')[0].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.4 s, sys: 57.6 ms, total: 48.5 s\n",
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(num_rows):\n",
    "    dep_delay_hr[i] = df.iloc[i, col_index]/60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 s, sys: 172 ms, total: 1.52 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dep_delay_hr_apply = df['DepDelay'].apply(lambda x: x/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 ms ± 6.42 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "delay_hr_apply = df[['DepDelay', 'ArrDelay']].apply(lambda x: x/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5 s ± 50.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "delay_hr_applymap = df[['DepDelay', 'ArrDelay']].applymap(lambda x: x/60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for converting minutes to hours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Goal** [as in Class 2](https://github.com/ikukuyeva/Stats-404-W19-Statistical-Computing/blob/master/Class2/Intro-to-pandas.ipynb): Convert continuous variable into categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_departure_delays(delay_min):\n",
    "    if delay_min <= 15:\n",
    "        return \"no_delay\"\n",
    "    elif (delay_min > 15) & (delay_min <= 30):\n",
    "        return \"small_delay\"\n",
    "    elif (delay_min > 30) & (delay_min <= 60):\n",
    "        return \"medium_delay\"        \n",
    "    elif (delay_min > 60) & (delay_min <= 120):\n",
    "        return \"big_delay\"        \n",
    "    elif (delay_min > 120):\n",
    "        return \"compensated_delay\"        \n",
    "    else:\n",
    "        return \"missing_delay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.79 s, sys: 79.4 ms, total: 1.87 s\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "delay_bin = df['DepDelay'].apply(lambda x: bin_departure_delays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_delay             5067251\n",
       "small_delay           366563\n",
       "medium_delay          292710\n",
       "big_delay             195039\n",
       "compensated_delay      99476\n",
       "missing_delay          75723\n",
       "Name: DepDelay, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_bin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 232 ms, sys: 39.5 ms, total: 272 ms\n",
      "Wall time: 270 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['DepDelay'] = df['DepDelay'].fillna(9999)\n",
    "delay_bin_cut = pd.cut(df['DepDelay'],\n",
    "                       bins=[-10000, 15, 30, 60, 120, 3000, 10000],\n",
    "                       labels=[\"no_delay\", \"small_delay\", \"medium_delay\", \"big_delay\", \"compensated_delay\", \"missing_delay\"]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_delay             5067251\n",
       "small_delay           366563\n",
       "medium_delay          292710\n",
       "big_delay             195039\n",
       "compensated_delay      99476\n",
       "missing_delay          75723\n",
       "Name: DepDelay, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_bin_cut.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More [examples](https://realpython.com/fast-flexible-pandas/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for binning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### b. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual explanation of [vectorization:](https://datascience.blog.wzb.eu/2018/02/02/vectorization-and-parallelization-in-python-with-numpy-and-pandas/)\n",
    "\n",
    "![Visual explanation of what vectorization is](./images/vectorization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Goal** [As in Class 3](https://github.com/ikukuyeva/Stats-404-W19-Statistical-Computing/blob/master/Class3/Intro-to-sklearn.ipynb): Create outcome variable for compensated delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delays_requiring_compensation(arrival_delay, departure_delay):\n",
    "    \"\"\"Fcn to return if arrival and/or departure delay resulted in passenger\n",
    "       compensation.\n",
    "       \n",
    "       Arguments:\n",
    "           - arrival_delay:   delay in minutes\n",
    "           - departure_delay: delay in minutes\n",
    "       \n",
    "       Returns:\n",
    "           - number of delays (arrival and or departure) that were delayed\n",
    "             so long that passenger got compensated\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    if (arrival_delay/60.0 >= 3) | (departure_delay/60.0 >= 2):\n",
    "        # If arrival delay is 3+ hours, or if departure delay is 2+ hours:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 23s, sys: 389 ms, total: 2min 24s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['compensated_delays'] = df[['ArrDelay', 'DepDelay']].apply(\n",
    "    lambda row: delays_requiring_compensation(row[0], row[1]),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5919407, 1: 177355})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['compensated_delays'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prerequisite for vectorizing with Boolean logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(True | True)\n",
    "print(True | False)\n",
    "print(False | True)\n",
    "print(False | False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def delays_requiring_compensation_vec(arrival_delay, departure_delay):\n",
    "    \"\"\"Fcn to return if arrival and/or departure delay resulted in passenger\n",
    "       compensation.\n",
    "       \n",
    "       Arguments:\n",
    "           - arrival_delay:   delay in minutes\n",
    "           - departure_delay: delay in minutes\n",
    "       \n",
    "       Returns:\n",
    "           - number of delays (arrival and or departure) that were delayed\n",
    "             so long that passenger got compensated\n",
    "    \"\"\"\n",
    "    count_arrival_delays = arrival_delay >= (3 * 60.0)\n",
    "    count_depaprture_delays = departure_delay >= (2 * 60.0)\n",
    "    # Leveraging Boolean logic:\n",
    "    compensated_delays = count_arrival_delays | count_depaprture_delays\n",
    "    return compensated_delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 ms, sys: 7.67 ms, total: 25.5 ms\n",
      "Wall time: 23.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['compensated_delays_vec'] = delays_requiring_compensation_vec(df['ArrDelay'], df['DepDelay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 5919407, True: 177355})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['compensated_delays_vec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More [examples](https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### c. `numpy` Operations via `.values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['ArrDelay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['ArrDelay'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 ms, sys: 4.6 ms, total: 18.5 ms\n",
      "Wall time: 15.8 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irina/Documents/Stats_404_W19/Envs/env-stats404-class0/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['compensated_delays_vec_np'] = delays_requiring_compensation_vec(df['ArrDelay'].values, df['DepDelay'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 5919407, True: 177355})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['compensated_delays_vec_np'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples: [here](https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6) and [here](https://jakevdp.github.io/PythonDataScienceHandbook/02.04-computation-on-arrays-aggregates.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for processing multiple columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. In-database Computations\n",
    "Please see Section \"Speeding-up Data Read\" (above) for more information and caveats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. `dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34 s, sys: 11.2 s, total: 45.3 s\n",
      "Wall time: 5min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WN    1140535\n",
       "EV     740855\n",
       "DL     726879\n",
       "OO     617756\n",
       "UA     531245\n",
       "AA     525220\n",
       "MQ     473140\n",
       "US     404263\n",
       "B6     229056\n",
       "FL     218162\n",
       "AS     147569\n",
       "YV     133976\n",
       "F9      79255\n",
       "HA      74109\n",
       "VX      54742\n",
       "Name: UniqueCarrier, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_dask['UniqueCarrier'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Warning](./images/warning.png) Per [bug submission](https://github.com/dask/dask/issues/442), while Dask's `value_counts()` [documentation](http://docs.dask.org/en/latest/dataframe-api.html) states that you can sort results as in `pandas`, Dask does not have that functionality implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. `pySpark` + `SparkSQL`\n",
    "Please go this [Databricks notebook](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7565198439641616/2474892018501188/7912616974346672/latest.html) to view `SparkSQL` code that performs counts by airline carrier for Airlines dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time duration (seconds) = 7.35 seconds (0.05 s to specify that we'll be running SparkSQL against dataset, 7.33 s to perform aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for getting number of flight paths by carrier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speeding-up Embarrassingly Parallel Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Class 4](https://github.com/ikukuyeva/Stats-404-W19-Statistical-Computing/blob/master/Class4/Fashion-MNIST.ipynb): We estimated 7 different Random Forest models in serial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to repository on my machine:\n",
    "fashion_mnist_dir = \"/Users/irina/Documents/Stats-Related/Fashion-MNIST-repo\"\n",
    "os.chdir(fashion_mnist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST data set using helper function from Fashion-MNIST repository:\n",
    "from utils import mnist_reader\n",
    "# Load 10K images for this demo:\n",
    "X, y = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   27.1s finished\n"
     ]
    }
   ],
   "source": [
    "rf_base = RandomForestClassifier(n_estimators=500,\n",
    "                                 min_samples_leaf=30,\n",
    "                                 oob_score=True,\n",
    "                                 random_state=2019,\n",
    "                                 class_weight='balanced',\n",
    "                                 verbose=1).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### a. (Potentially) Leverage Parralel Backend of `sklearn` model\n",
    "Example: `sklearn` RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per sklearn documentation of [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html):\n",
    "\n",
    "`n_jobs` : int or None, optional (default=None)\n",
    "\n",
    "The number of jobs to run in parallel for **both fit and predict**. None means 1 unless in a joblib.parallel_backend context. \n",
    "-1 means using all processors...\n",
    "\n",
    "Note: Parallel backend is [joblib](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=2)]: Done 500 out of 500 | elapsed:   13.8s finished\n"
     ]
    }
   ],
   "source": [
    "rf_base_parallel2 = RandomForestClassifier(n_estimators=500,\n",
    "                                 min_samples_leaf=30,\n",
    "                                 oob_score=True,\n",
    "                                 random_state=2019,\n",
    "                                 class_weight='balanced',\n",
    "                                 verbose=1,\n",
    "                                 n_jobs=2).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much speed-up did we get by using 2 cores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    7.3s finished\n"
     ]
    }
   ],
   "source": [
    "rf_base_parallel4 = RandomForestClassifier(n_estimators=500,\n",
    "                                 min_samples_leaf=30,\n",
    "                                 oob_score=True,\n",
    "                                 random_state=2019,\n",
    "                                 class_weight='balanced',\n",
    "                                 verbose=1,\n",
    "                                 n_jobs=4).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is speed-up not 4x?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### b. `joblib` for Embarrassingly Parallel Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_spec(num_trees, features=X, outcome=y):\n",
    "    ### --- RF model to estimate:\n",
    "    rf = RandomForestClassifier(n_estimators=num_trees,\n",
    "                                min_samples_leaf=30,\n",
    "                                oob_score=True,\n",
    "                                random_state=2019,\n",
    "                                class_weight='balanced',\n",
    "                                verbose=1)\n",
    "    ### --- Estimate RF model and save estimated model:\n",
    "    rf.fit(features, outcome)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = [50, 100, 250, 500, 1000, 1500, 2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Baseline for Estimating 7 RFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:   13.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   28.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   55.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1500 out of 1500 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2500 out of 2500 | elapsed:  2.3min finished\n"
     ]
    }
   ],
   "source": [
    "for num_trees in n_trees:\n",
    "    rf_spec(num_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Leveraging Parallelization to Estimate Forests Simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:   16.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   32.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1500 out of 1500 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Done 2500 out of 2500 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=4)]: Done   7 out of   7 | elapsed:  3.0min finished\n"
     ]
    }
   ],
   "source": [
    "# Per http://academic.bancey.com/parallelization-in-python-example-with-joblib/\n",
    "results = Parallel(n_jobs=4, verbose=1, backend=\"threading\")(map(delayed(rf_spec), n_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Which is fastest for parallelizing RF model estimation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Aside: [Explanation](https://stackoverflow.com/questions/42220458/what-does-the-delayed-function-do-when-used-with-joblib-in-python) of `delayed` argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no clear tech stack winner for which solution will speed-up computations for each use case; speed depends on:\n",
    "  - size of dataset\n",
    "  - analyses you want to perform\n",
    "  - computing architecture\n",
    "  - (many others)\n",
    " \n",
    " \n",
    "- Airlines data set (using 2012 flight paths only) might be too small for our pySpark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further Reading\n",
    "- [High Performance Python](http://shop.oreilly.com/product/0636920028963.do)\n",
    "  - Appropriate usage of lists vs tuples \n",
    "  - Iterators and Generators\n",
    "  - Compiling to C\n",
    "  - (more on) Cluster Computing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-stats404-class0",
   "language": "python",
   "name": "env-stats404-class0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
